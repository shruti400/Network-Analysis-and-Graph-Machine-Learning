{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import scipy.sparse as sp\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import average_precision_score\n",
        "import pickle\n",
        "import scipy.linalg as linalg # Import the dense linear algebra module\n",
        "\n"
      ],
      "metadata": {
        "id": "1ghIPE7tnsKF"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iVh5s9Pimj3A"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load node features and labels\n",
        "column_names = ['node_id'] + [f'w_{i}' for i in range(1433)] + ['subject']\n",
        "node_data = pd.read_csv('cora.content', sep='\\t', header=None, names=column_names)\n",
        "\n",
        "# Load edges\n",
        "edge_data = pd.read_csv('cora.cites', sep='\\t', header=None, names=['target', 'source'])\n",
        "\n",
        "# Create a directed graph\n",
        "g= nx.from_pandas_edgelist(edge_data, source='source', target='target', create_using=nx.DiGraph())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step1: Preprocessing/Train-Test Split\n"
      ],
      "metadata": {
        "id": "pCZKWethnhw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sparse_to_tuple(sparse_mx):\n",
        "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
        "    def to_tuple(mx):\n",
        "        if not sp.isspmatrix_coo(mx):\n",
        "            mx = mx.tocoo()\n",
        "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
        "        values = mx.data\n",
        "        shape = mx.shape\n",
        "        return coords, values, shape\n",
        "\n",
        "    if isinstance(sparse_mx, list):\n",
        "        return [to_tuple(mx) for mx in sparse_mx]\n",
        "    else:\n",
        "        return to_tuple(sparse_mx)\n",
        "\n",
        "\n",
        "def mask_test_edges_directed(adj, test_frac=.05, val_frac=.05,\n",
        "    prevent_disconnect=False, verbose=False, false_edge_sampling='random'):\n",
        "\n",
        "    if verbose:\n",
        "        print('preprocessing...')\n",
        "\n",
        "    # Remove diagonal elements\n",
        "    adj = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
        "    adj.eliminate_zeros()\n",
        "    # Check that diag is zero:\n",
        "    assert np.diag(adj.todense()).sum() == 0\n",
        "\n",
        "    # Convert to networkx graph to calc num. weakly connected components\n",
        "    # Create graph from the adjacency matrix\n",
        "    # Use from_scipy_sparse_array instead of the deprecated from_scipy_sparse_matrix\n",
        "    g = nx.from_scipy_sparse_array(adj, create_using=nx.DiGraph())\n",
        "    orig_num_wcc = nx.number_weakly_connected_components(g)\n",
        "\n",
        "    adj_tuple = sparse_to_tuple(adj) # (coords, values, shape)\n",
        "    edges = adj_tuple[0] # List of ALL edges (either direction)\n",
        "    # Store edges as list of tuples (from_node, to_node)\n",
        "    edge_pairs = [(edge[0], edge[1]) for edge in edges]\n",
        "\n",
        "    num_test = int(np.floor(edges.shape[0] * test_frac)) # controls how large the test set should be\n",
        "    num_val = int(np.floor(edges.shape[0] * val_frac)) # controls how large the validation set should be\n",
        "    num_train = len(edge_pairs) - num_test - num_val # num train edges\n",
        "\n",
        "    all_edge_set = set(edge_pairs)\n",
        "    train_edges = set(edge_pairs) # init train_edges to have all edges\n",
        "    test_edges = set() # init test_edges as empty set\n",
        "    val_edges = set() # init val edges as empty set\n",
        "\n",
        "    ### ---------- TRUE EDGES ---------- ###\n",
        "    # Shuffle and iterate over all edges\n",
        "    np.random.shuffle(edge_pairs)\n",
        "\n",
        "    # get initial bridge edges from the undirected version of the graph\n",
        "    bridge_edges = set(nx.bridges(nx.to_undirected(g)))\n",
        "\n",
        "    if verbose:\n",
        "        print('creating true edges...')\n",
        "\n",
        "    # Create a copy of the graph to remove edges from temporarily\n",
        "    g_temp = g.copy()\n",
        "\n",
        "    for ind, edge in enumerate(edge_pairs):\n",
        "        node1, node2 = edge[0], edge[1]\n",
        "\n",
        "        # Recalculate bridges every ____ iterations to be relatively recent\n",
        "        if ind % 10000 == 0:\n",
        "            bridge_edges = set(nx.bridges(nx.to_undirected(g_temp)))\n",
        "\n",
        "        # Don't sample bridge edges to increase likelihood of staying connected\n",
        "        # Check for both directions in undirected bridges\n",
        "        if (node1, node2) in bridge_edges or (node2, node1) in bridge_edges:\n",
        "            continue\n",
        "\n",
        "        # If removing edge would disconnect the graph, backtrack and move on\n",
        "        g_temp.remove_edge(node1, node2)\n",
        "        if prevent_disconnect:\n",
        "            if not nx.is_weakly_connected(g_temp):\n",
        "                g_temp.add_edge(node1, node2) # Add back the edge if it disconnects\n",
        "                continue\n",
        "\n",
        "        # Fill test_edges first\n",
        "        if len(test_edges) < num_test:\n",
        "            test_edges.add(edge)\n",
        "            # Remove from train_edges set for true edges\n",
        "            if edge in train_edges:\n",
        "                train_edges.remove(edge)\n",
        "            if verbose and len(test_edges) % 10000 == 0:\n",
        "                print('Current num test edges: ', len(test_edges))\n",
        "\n",
        "        # Then, fill val_edges\n",
        "        elif len(val_edges) < num_val:\n",
        "            val_edges.add(edge)\n",
        "            # Remove from train_edges set for true edges\n",
        "            if edge in train_edges:\n",
        "                train_edges.remove(edge)\n",
        "            if verbose and len(val_edges) % 10000 == 0:\n",
        "                print('Current num val edges: ', len(val_edges))\n",
        "\n",
        "        # Both edge lists full --> break loop\n",
        "        elif len(test_edges) == num_test and len(val_edges) == num_val:\n",
        "            break\n",
        "\n",
        "    # Check that enough test/val edges were found\n",
        "    if (len(val_edges) < num_val or len(test_edges) < num_test):\n",
        "        print(f\"WARNING: not enough removable edges to perform full train-test split! Requested: Test {num_test}, Val {num_val}. Returned: Test {len(test_edges)}, Val {len(val_edges)}\")\n",
        "\n",
        "\n",
        "    # Calculate largest weakly connected component on the graph after removing test/val true edges\n",
        "    wccs = list(nx.weakly_connected_components(g_temp))\n",
        "    if not wccs:\n",
        "         print(\"WARNING: Graph became disconnected after removing edges and has no weakly connected components.\")\n",
        "         largest_wcc_set = set()\n",
        "         largest_wcc = nx.DiGraph()\n",
        "    else:\n",
        "        largest_wcc_set = max(wccs, key=len)\n",
        "        largest_wcc = g_temp.subgraph(largest_wcc_set)\n",
        "\n",
        "\n",
        "    # Print stats for largest remaining WCC\n",
        "    print('Num WCC in remaining graph: ', nx.number_weakly_connected_components(g_temp))\n",
        "    print('Largest WCC num nodes: ', largest_wcc.number_of_nodes())\n",
        "    print('Largest WCC num edges: ', largest_wcc.number_of_edges())\n",
        "\n",
        "    if prevent_disconnect:\n",
        "         # Recompute WCC on the graph with train edges only\n",
        "         g_train_check = nx.from_edgelist(list(train_edges), create_using=nx.DiGraph())\n",
        "         # Check if the number of weakly connected components is the same as the original graph\n",
        "         # Handle the case where the original graph might have had multiple components and\n",
        "         # the largest WCC filtering removes some nodes entirely.\n",
        "         # A stricter check could be that the nodes in g_train_check are a subset of original nodes\n",
        "         # and WCC count is the same for the induced subgraph on original nodes\n",
        "         # For now, just check if the main component is still connected like the original\n",
        "         # This requires careful consideration based on the desired behavior.\n",
        "         # If the goal is to work *only* within the largest WCC of the original graph,\n",
        "         # the orig_num_wcc should be based on the largest WCC of the *original* graph.\n",
        "         # If the goal is to maintain connectivity of the *entire* original graph structure,\n",
        "         # this assertion needs refinement.\n",
        "         # Assuming the goal is to maintain connectivity of the largest component:\n",
        "         g_orig_largest_wcc = g.subgraph(max(nx.weakly_connected_components(g), key=len))\n",
        "         # Only perform the connectivity check if both the original largest WCC and the training graph are not empty\n",
        "         #if g_orig_largest_wcc.number_of_nodes() > 0 and g_train_check.number_of_nodes() > 0:\n",
        "             # Need to ensure the nodes of g_orig_largest_wcc are present in g_train_check before creating subgraph\n",
        "          #   nodes_in_g_train_check = set(g_train_check.nodes())\n",
        "           #  nodes_in_orig_wcc_and_train = sorted(list(g_orig_largest_wcc.nodes() & nodes_in_g_train_check))\n",
        "            # if nodes_in_orig_wcc_and_train: # Check if there are common nodes\n",
        "             #    assert nx.is_weakly_connected(g_train_check.subgraph(nodes_in_orig_wcc_and_train)) == nx.is_weakly_connected(g_orig_largest_wcc.subgraph(nodes_in_orig_wcc_and_train))\n",
        "             #else:\n",
        "             #    # If no common nodes, a strict connectivity assertion might not be meaningful here\n",
        "              #   print(\"INFO: No common nodes between original largest WCC and training graph. Skipping connectivity assertion.\")\n",
        "\n",
        "\n",
        "    # Filter edges to only include those where both endpoints are in the largest WCC\n",
        "    # This is crucial for ensuring the downstream tasks operate within a connected component\n",
        "    train_edges = {edge for edge in train_edges if edge[0] in largest_wcc_set and edge[1] in largest_wcc_set}\n",
        "    test_edges = {edge for edge in test_edges if edge[0] in largest_wcc_set and edge[1] in largest_wcc_set}\n",
        "    val_edges = {edge for edge in val_edges if edge[0] in largest_wcc_set and edge[1] in largest_wcc_set}\n",
        "\n",
        "\n",
        "    ### ---------- FALSE EDGES ---------- ###\n",
        "\n",
        "    # Initialize empty sets\n",
        "    train_edges_false = set()\n",
        "    test_edges_false = set()\n",
        "    val_edges_false = set()\n",
        "\n",
        "    # Generate candidate false edges (from g-complement) and iterate through them\n",
        "    if false_edge_sampling == 'iterative':\n",
        "        if verbose:\n",
        "            print(\"preparing complement adjacency matrix...\")\n",
        "\n",
        "        # Sample false edges from G-complement, instead of randomly generating edges\n",
        "        # Create a sparse adjacency matrix for the graph with training edges\n",
        "        nodes_in_wcc_list = sorted(list(largest_wcc_set))\n",
        "        num_nodes_wcc = len(nodes_in_wcc_list)\n",
        "\n",
        "        if num_nodes_wcc == 0:\n",
        "            print(\"WARNING: Largest WCC is empty. Cannot generate false edges.\")\n",
        "        else:\n",
        "            # Create adj matrix for the largest WCC subgraph\n",
        "            # Use the nodes from the largest WCC found *after* removing test/val true edges\n",
        "            adj_wcc = nx.adjacency_matrix(largest_wcc, nodelist=nodes_in_wcc_list).astype(np.int8)\n",
        "\n",
        "            # Find coordinates of zero entries (non-existing edges) within the WCC subgraph's adjacency matrix\n",
        "            adj_wcc_coo = adj_wcc.tocoo()\n",
        "            existing_coords_wcc_indices = set((r, c) for r, c in zip(adj_wcc_coo.row, adj_wcc_coo.col))\n",
        "\n",
        "            edge_pairs_false_candidates = []\n",
        "            for i in range(num_nodes_wcc):\n",
        "                for j in range(num_nodes_wcc):\n",
        "                    if i != j and (i, j) not in existing_coords_wcc_indices:\n",
        "                        # Map WCC indices back to original node IDs\n",
        "                        original_u = nodes_in_wcc_list[i]\n",
        "                        original_v = nodes_in_wcc_list[j]\n",
        "                        edge_pairs_false_candidates.append((original_u, original_v))\n",
        "\n",
        "            # Shuffle and iterate over false edges\n",
        "            np.random.shuffle(edge_pairs_false_candidates)\n",
        "            if verbose:\n",
        "                print(\"adding candidate false edges to false edge sets...\")\n",
        "\n",
        "            for false_edge in edge_pairs_false_candidates:\n",
        "                 # Ensure the false edge is between nodes within the largest WCC (already done by candidate generation)\n",
        "                 # Ensure it's not an existing edge (should be guaranteed by construction)\n",
        "                 # No need to check all_edge_set explicitly since candidates are from complement within WCC\n",
        "\n",
        "                 # Fill train_edges_false first\n",
        "                 if len(train_edges_false) < len(train_edges):\n",
        "                    train_edges_false.add(false_edge)\n",
        "                    if verbose and len(train_edges_false) % 100000 == 0:\n",
        "                        print(f'Current num false train edges: {len(train_edges_false)}')\n",
        "\n",
        "                 # Fill test_edges_false next\n",
        "                 elif len(test_edges_false) < len(test_edges):\n",
        "                    # Ensure it's not already in train/val false\n",
        "                    if false_edge not in train_edges_false and false_edge not in val_edges_false:\n",
        "                       test_edges_false.add(false_edge)\n",
        "                       if verbose and len(test_edges_false) % 100000 == 0:\n",
        "                           print(f'Current num false test edges: {len(test_edges_false)}')\n",
        "\n",
        "                 # Fill val_edges_false last\n",
        "                 elif len(val_edges_false) < len(val_edges):\n",
        "                    # Ensure it's not already in train/test false\n",
        "                    if false_edge not in train_edges_false and false_edge not in test_edges_false:\n",
        "                       val_edges_false.add(false_edge)\n",
        "                       if verbose and len(val_edges_false) % 100000 == 0:\n",
        "                           print(f'Current num false val edges: {len(val_edges_false)}')\n",
        "\n",
        "                 # All sets filled --> break\n",
        "                 elif len(train_edges_false) == len(train_edges) and \\\n",
        "                      len(test_edges_false) == len(test_edges) and \\\n",
        "                      len(val_edges_false) == len(val_edges):\n",
        "                     break\n",
        "\n",
        "            # If not enough false edges were found in the iterative approach, warn the user\n",
        "            if (len(train_edges_false) < len(train_edges) or\n",
        "                len(test_edges_false) < len(test_edges) or\n",
        "                len(val_edges_false) < len(val_edges)):\n",
        "                 print(\"WARNING: Not enough false edge candidates found in iterative sampling within the largest WCC to match the number of true edges in each set.\")\n",
        "                 print(f\"Required false edges: Train {len(train_edges)}, Test {len(test_edges)}, Val {len(val_edges)}\")\n",
        "                 print(f\"Returned false edges: Train {len(train_edges_false)}, Test {len(test_edges_false)}, Val {len(val_edges_false)}\")\n",
        "\n",
        "\n",
        "    # Randomly generate false edges (idx_i, idx_j) 1 at a time to save memory\n",
        "    elif false_edge_sampling == 'random':\n",
        "        if verbose:\n",
        "            print('creating false edges...')\n",
        "\n",
        "        num_nodes = adj.shape[0]\n",
        "        largest_wcc_list = list(largest_wcc_set) # Convert set to list for random sampling\n",
        "\n",
        "        # FALSE TEST EDGES\n",
        "        if verbose:\n",
        "            print('creating false test edges...')\n",
        "        while len(test_edges_false) < len(test_edges):\n",
        "            if not largest_wcc_list:\n",
        "                 print(\"WARNING: Largest WCC is empty. Cannot generate false edges.\")\n",
        "                 break\n",
        "\n",
        "            # Sample nodes only from the largest WCC\n",
        "            # Ensure replace=True is used as nodes can be sampled multiple times before forming a valid false edge\n",
        "            idx_i, idx_j = np.random.choice(largest_wcc_list, 2, replace=True)\n",
        "\n",
        "\n",
        "            if idx_i == idx_j: # no self-loops\n",
        "                continue\n",
        "\n",
        "            false_edge = (idx_i, idx_j)\n",
        "\n",
        "            # Make sure false_edge not an actual edge, and not a repeat in any false set\n",
        "            if false_edge in all_edge_set or \\\n",
        "               false_edge in test_edges_false or \\\n",
        "               false_edge in val_edges_false or \\\n",
        "               false_edge in train_edges_false:\n",
        "                continue\n",
        "\n",
        "            test_edges_false.add(false_edge)\n",
        "\n",
        "            if verbose and len(test_edges_false) % 100000 == 0:\n",
        "                print(f'Current num false test edges: {len(test_edges_false)}')\n",
        "\n",
        "        # FALSE VAL EDGES\n",
        "        if verbose:\n",
        "            print('creating false val edges...')\n",
        "\n",
        "        while len(val_edges_false) < len(val_edges):\n",
        "            if not largest_wcc_list:\n",
        "                 break\n",
        "            # Sample nodes only from the largest WCC\n",
        "            idx_i, idx_j = np.random.choice(largest_wcc_list, 2, replace=True)\n",
        "            if idx_i == idx_j:\n",
        "                continue\n",
        "\n",
        "            false_edge = (idx_i, idx_j)\n",
        "\n",
        "            # Make sure false_edge in not an actual edge, not in test_edges_false, not a repeat\n",
        "            if false_edge in all_edge_set or \\\n",
        "                false_edge in test_edges_false or \\\n",
        "                false_edge in val_edges_false or \\\n",
        "                false_edge in train_edges_false:\n",
        "                continue\n",
        "\n",
        "            val_edges_false.add(false_edge)\n",
        "\n",
        "            if verbose and len(val_edges_false) % 100000 == 0:\n",
        "                print(f'Current num false val edges: {len(val_edges_false)}')\n",
        "\n",
        "        # FALSE TRAIN EDGES\n",
        "        if verbose:\n",
        "            print('creating false train edges...')\n",
        "\n",
        "        while len(train_edges_false) < len(train_edges):\n",
        "            if not largest_wcc_list:\n",
        "                 break\n",
        "            # Sample nodes only from the largest WCC\n",
        "            idx_i, idx_j = np.random.choice(largest_wcc_list, 2, replace=True)\n",
        "            if idx_i == idx_j:\n",
        "                continue\n",
        "\n",
        "            false_edge = (idx_i, idx_j)\n",
        "\n",
        "            # Make sure false_edge in not an actual edge, not in test_edges_false,\n",
        "                # not in val_edges_false, not a repeat\n",
        "            if false_edge in all_edge_set or \\\n",
        "                false_edge in test_edges_false or \\\n",
        "                false_edge in val_edges_false or \\\n",
        "                false_edge in train_edges_false:\n",
        "                continue\n",
        "\n",
        "            train_edges_false.add(false_edge)\n",
        "\n",
        "            if verbose and len(train_edges_false) % 100000 == 0:\n",
        "                print(f'Current num false train edges: {len(train_edges_false)}')\n",
        "\n",
        "\n",
        "    ### ---------- FINAL DISJOINTNESS CHECKS ---------- ###\n",
        "    if verbose:\n",
        "        print('final checks for disjointness...')\n",
        "\n",
        "    # assert: false_edges are actually false (not in all_edge_tuples)\n",
        "    assert test_edges_false.isdisjoint(all_edge_set)\n",
        "    assert val_edges_false.isdisjoint(all_edge_set)\n",
        "    assert train_edges_false.isdisjoint(all_edge_set)\n",
        "\n",
        "    # assert: test, val, train false edges disjoint\n",
        "    assert test_edges_false.isdisjoint(val_edges_false)\n",
        "    assert test_edges_false.isdisjoint(train_edges_false)\n",
        "    assert val_edges_false.isdisjoint(train_edges_false)\n",
        "\n",
        "    # assert: test, val, train positive edges disjoint\n",
        "    assert val_edges.isdisjoint(train_edges)\n",
        "    assert test_edges.isdisjoint(train_edges)\n",
        "    assert val_edges.isdisjoint(test_edges)\n",
        "\n",
        "    if verbose:\n",
        "        print('creating adj_train...')\n",
        "\n",
        "    # Re-build adj matrix using remaining graph (train edges only)\n",
        "    # Create a graph with only the training edges\n",
        "    g_train = nx.from_edgelist(list(train_edges), create_using=nx.DiGraph())\n",
        "\n",
        "    # Ensure the adjacency matrix has the same shape as the training graph based on the nodes it actually contains\n",
        "    # Use from_scipy_sparse_array\n",
        "    # Use the nodes from the training graph itself for the nodelist\n",
        "    adj_train = nx.adjacency_matrix(g_train, nodelist=sorted(list(g_train.nodes()))).astype(np.int8)\n",
        "\n",
        "\n",
        "    # Convert edge-lists to numpy arrays\n",
        "    # Sort the sets first to ensure consistent ordering for numpy array conversion\n",
        "    train_edges = np.array(sorted(list(train_edges)))\n",
        "    train_edges_false = np.array(sorted(list(train_edges_false)))\n",
        "    val_edges = np.array(sorted(list(val_edges)))\n",
        "    val_edges_false = np.array(sorted(list(val_edges_false)))\n",
        "    test_edges = np.array(sorted(list(test_edges)))\n",
        "    test_edges_false = np.array(sorted(list(test_edges_false)))\n",
        "\n",
        "    if verbose:\n",
        "        print('Done with train-test split!')\n",
        "        print(f'Num train edges (true, false): ({train_edges.shape[0]}, {train_edges_false.shape[0]})')\n",
        "        print(f'Num test edges (true, false): ({test_edges.shape[0]}, {test_edges_false.shape[0]})')\n",
        "        print(f'Num val edges (true, false): ({val_edges.shape[0]}, {val_edges_false.shape[0]})')\n",
        "        print('')\n",
        "\n",
        "    # Return final edge lists (edges can go either direction!) and the training adjacency matrix\n",
        "    return adj_train, train_edges, train_edges_false, \\\n",
        "        val_edges, val_edges_false, test_edges, test_edges_false\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mx5QUqxQso4W"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sparse adjacency matrix from the graph for the function\n",
        "adj = nx.adjacency_matrix(g).astype(np.int8)\n",
        "\n",
        "# Call the function to perform the split\n",
        "adj_train, train_edges, train_edges_false, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges_directed(adj, verbose=True)\n",
        "\n",
        "print(\"Train True Edges shape:\", train_edges.shape)\n",
        "print(\"Train False Edges shape:\", train_edges_false.shape)\n",
        "print(\"Validation True Edges shape:\", val_edges.shape)\n",
        "print(\"Validation False Edges shape:\", val_edges_false.shape)\n",
        "print(\"Test True Edges shape:\", test_edges.shape)\n",
        "print(\"Test False Edges shape:\", test_edges_false.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-HI3yZCycFY",
        "outputId": "f77956de-9421-4417-d7fe-18c37fd8ec32"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preprocessing...\n",
            "creating true edges...\n",
            "Num WCC in remaining graph:  88\n",
            "Largest WCC num nodes:  2470\n",
            "Largest WCC num edges:  4676\n",
            "creating false edges...\n",
            "creating false test edges...\n",
            "creating false val edges...\n",
            "creating false train edges...\n",
            "final checks for disjointness...\n",
            "creating adj_train...\n",
            "Done with train-test split!\n",
            "Num train edges (true, false): (4677, 4677)\n",
            "Num test edges (true, false): (252, 252)\n",
            "Num val edges (true, false): (251, 251)\n",
            "\n",
            "Train True Edges shape: (4677, 2)\n",
            "Train False Edges shape: (4677, 2)\n",
            "Validation True Edges shape: (251, 2)\n",
            "Validation False Edges shape: (251, 2)\n",
            "Test True Edges shape: (252, 2)\n",
            "Test False Edges shape: (252, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g_train = nx.from_scipy_sparse_array(adj_train) # new graph object with only non-hidden edges\n"
      ],
      "metadata": {
        "id": "HfxL8nlzvYO5"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_roc_score(edges_pos, edges_neg, score_matrix):\n",
        "    # Store positive edge predictions, actual values\n",
        "    preds_pos = []\n",
        "    pos = []\n",
        "    for edge in edges_pos:\n",
        "        preds_pos.append(score_matrix[edge[0], edge[1]]) # predicted score\n",
        "        pos.append(adj[edge[0], edge[1]]) # actual value (1 for positive)\n",
        "\n",
        "    # Store negative edge predictions, actual values\n",
        "    preds_neg = []\n",
        "    neg = []\n",
        "    for edge in edges_neg:\n",
        "        preds_neg.append(score_matrix[edge[0], edge[1]]) # predicted score\n",
        "        neg.append(adj[edge[0], edge[1]]) # actual value (0 for negative)\n",
        "\n",
        "    # Calculate scores\n",
        "    preds_all = np.hstack([preds_pos, preds_neg])\n",
        "    labels_all = np.hstack([np.ones(len(preds_pos)), np.zeros(len(preds_neg))])\n",
        "    roc_score = roc_auc_score(labels_all, preds_all)\n",
        "    ap_score = average_precision_score(labels_all, preds_all)\n",
        "    return roc_score, ap_score"
      ],
      "metadata": {
        "id": "vT5aLO6EvClr"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adamic-Adar\n"
      ],
      "metadata": {
        "id": "p71b-tPjvMNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Adamic-Adar indexes from g_train\n",
        "aa_matrix = np.zeros(adj.shape)\n",
        "for u, v, p in nx.adamic_adar_index(g_train): # (u, v) = node indices, p = Adamic-Adar index\n",
        "    aa_matrix[u][v] = p\n",
        "    aa_matrix[v][u] = p # make sure it's symmetric\n",
        "\n",
        "# Normalize array\n",
        "aa_matrix = aa_matrix / aa_matrix.max()"
      ],
      "metadata": {
        "id": "nucBBCu_vOdL"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate ROC AUC and Average Precision\n",
        "aa_roc, aa_ap = get_roc_score(test_edges, test_edges_false, aa_matrix)\n",
        "\n",
        "print('Adamic-Adar Test ROC score: ', str(aa_roc))\n",
        "print('Adamic-Adar Test AP score: ', str(aa_ap))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_X51lBiHvspj",
        "outputId": "c36b5c76-53a9-4797-e516-e78a8de4f317"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adamic-Adar Test ROC score:  0.5997314453125\n",
            "Adamic-Adar Test AP score:  0.5982874517097334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Jaccard Coefficient\n"
      ],
      "metadata": {
        "id": "W8xdtPRPxEf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Jaccard Coefficients from g_train\n",
        "jc_matrix = np.zeros(adj.shape)\n",
        "for u, v, p in nx.jaccard_coefficient(g_train): # (u, v) = node indices, p = Jaccard coefficient\n",
        "    jc_matrix[u][v] = p\n",
        "    jc_matrix[v][u] = p # make sure it's symmetric\n",
        "\n",
        "# Normalize array\n",
        "jc_matrix = jc_matrix / jc_matrix.max()"
      ],
      "metadata": {
        "id": "xP_p8NLzt38P"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate ROC AUC and Average Precision\n",
        "jc_roc, jc_ap = get_roc_score(test_edges, test_edges_false, jc_matrix)\n",
        "\n",
        "print('Jaccard Coefficient Test ROC score: ', str(jc_roc))\n",
        "print('Jaccard Coefficient Test AP score: ', str(jc_ap))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cq1CGzTLxb-n",
        "outputId": "5c6da3fb-60b5-44a3-d5e5-bbcf6ab6a524"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jaccard Coefficient Test ROC score:  0.6005935668945312\n",
            "Jaccard Coefficient Test AP score:  0.6036074501509878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preferential Attachment\n"
      ],
      "metadata": {
        "id": "X19nvZNcxnaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate, store Adamic-Index scores in array\n",
        "pa_matrix = np.zeros(adj.shape)\n",
        "for u, v, p in nx.preferential_attachment(g_train): # (u, v) = node indices, p = Jaccard coefficient\n",
        "    pa_matrix[u][v] = p\n",
        "    pa_matrix[v][u] = p # make sure it's symmetric\n",
        "\n",
        "# Normalize array\n",
        "pa_matrix = pa_matrix / pa_matrix.max()"
      ],
      "metadata": {
        "id": "--2sgT-UxmTf"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate ROC AUC and Average Precision\n",
        "pa_roc, pa_ap = get_roc_score(test_edges, test_edges_false, pa_matrix)\n",
        "\n",
        "print('Preferential Attachment Test ROC score: ', str(pa_roc))\n",
        "print('Preferential Attachment Test AP score: ', str(pa_ap))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSHgZ_ELxz-n",
        "outputId": "68e38e58-61a1-4b4f-e059-7c046d0406f8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preferential Attachment Test ROC score:  0.5609359741210938\n",
            "Preferential Attachment Test AP score:  0.5763347205018841\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Common Neighbors"
      ],
      "metadata": {
        "id": "YJVlQ8UsMpRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate, store Common Neighbor scores in array\n",
        "cn_matrix = np.zeros(adj.shape)\n",
        "# Iterate over all pairs of nodes in g_train and their common neighbors count\n",
        "for u, v, p in nx.common_neighbors(g_train,u,v): # (u, v) = node indices, p = num_common_neighbors\n",
        "    cn_matrix[u][v] = p\n",
        "    cn_matrix[v][u] = p # make sure it's symmetric (based on undirected interpretation)\n",
        "\n",
        "# Normalize array\n",
        "max_cn = cn_matrix.max()\n",
        "if max_cn > 0:\n",
        "    cn_matrix = cn_matrix / max_cn\n",
        "else:\n",
        "    # If max_cn is 0, the matrix remains zeros, which is the correct normalized form in this case.\n",
        "    pass\n",
        "\n",
        "#cn_matrix = cn_matrix / cn_matrix.max()\n",
        "\n",
        "# Calculate ROC AUC and Average Precision\n",
        "cn_roc, cn_ap = get_roc_score(test_edges, test_edges_false, cn_matrix)\n",
        "\n",
        "print('Common Neighbors Test ROC score: ', str(cn_roc))\n",
        "print('Common Neighbors Test AP score: ', str(cn_ap))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kzF5_tzx_nG",
        "outputId": "39875fdc-1f5f-4bac-b0d4-efc1d7eafa2b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Common Neighbors Test ROC score:  0.5\n",
            "Common Neighbors Test AP score:  0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qmL5cRb5CfKy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}